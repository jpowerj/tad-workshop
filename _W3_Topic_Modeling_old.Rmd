---
title: "Topic Modeling for Social Scientists"
author: "Jeff Jacobs"
date: "November 29, 2018"
output: html_document
bibliography: Topic_Modeling_files/Topic_Modeling.bib
---

# Topic Modeling Overview

## Why Are We Doing This?

Either because we want to gain insights into a text corpus (and subsequently test hypotheses) that's too big to read, or because the texts are really boring and you don't want to read them all (my case).

Although as social scientists our first instinct is often to immediately start running regressions, I would describe topic modeling more as a method of "exploratory data analysis", as opposed to statistical data analysis methods like regression.

What this means is, until we get to the Structural Topic Model (if it ever works), we won't be quantitatively *evaluating* hypotheses but rather viewing our dataset through different "lenses", hopefully *generating* testable hypotheses along the way.

There are [whole courses](https://www.coursera.org/learn/exploratory-data-analysis) and [textbooks written by famous scientists](https://books.google.com/books/about/Exploratory_Data_Analysis.html?id=UT9dAAAAIAAJ) devoted solely to Exploratory Data Analysis, so I won't try to reinvent the wheel here, but I do want to point out that topic modeling pretty clearly dispels the typical critique from the humanities and (some) social sciences that computational text analysis just "reduces everything down to numbers and algorithms" or "tries to quantify the unquantifiable" (or my favorite comment, "a computer can't read a book"). It simply transforms, summarizes, zooms in and out, or otherwise manipulates your data in a customizable manner, with the whole purpose being to help you gain insights you wouldn't have been able to develop otherwise. Note that this doesn't imply (a) that the human gets replaced in the pipeline (*you* have to set up the algorithms and *you* have to do the interpretation of their results), or (b) that the computer is able to solve every question humans pose to it.

## The Code

```{r,load-data}
# Ingest the data
library(topicmodels)
# In case of errors when installing topicmodels:
# http://tinyheero.github.io/2016/02/20/install-r-topicmodels.html
library(tm)
# For loading corpus from directory
#const_corpus <- Corpus(DirSource("constitutions"))
# For loading corpus from csv file
library(data.table)
UN_data <- fread("corpora/UNGDC_2017_ascii.csv", data.table = FALSE)
# Get it into tm
#blog_corpus <- Corpus(DataframeSource(blog_data))
# ok that didn't work, like everything else in R
UN_corpus <- Corpus(VectorSource(UN_data$text))
# Look at a doc
writeLines(as.character(UN_corpus[[30]]))
# Preprocessing
# Load in our "custom" preprocessing function
source("W3_files/preprocessing.r")
UN_clean <- do_preprocessing(UN_corpus)
#Create document-term matrix
UN_dtm <- DocumentTermMatrix(UN_clean)
```
Now here's what's different this time: we go past the DTM "stage" of the pipeline, instead using it as an *input* to the LDA stage:
```{r,lda-train,eval=FALSE}
# The actual model
K <- 20
UN_lda <-LDA(UN_dtm, K, method="Gibbs")
UN_topics <- as.matrix(topics(UN_lda))
write.csv(UN_topics,file=paste0("UN_",K,"_DocsToTopics.csv"))
UN_terms <- as.matrix(terms(UN_lda,6))
write.csv(UN_terms,file=paste0("UN_",K,"_TopicsToTerms.csv"))
topic_probs <- as.data.frame(UN_lda@gamma)
write.csv(topic_probs,file=paste0("UN_",K,"_TopicProbabilities.csv"))
```

Now some visualization
```{r,lda-viz,eval=FALSE}
## From here onwards I'm stealing from
## https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html
# Apparently we need to explicitly tell it to calculate posteriors
blog_result <- posterior(blog_lda)
# Good time to introduce "attributes" function
attributes(blog_result)
# Now we can get the top N terms for a given topic using
topic_num <- 1
top_40 <- sort(blog_result$terms[topic_num,], decreasing=TRUE)[1:40]
top_40_words <- names(top_40)
print(top_40_words)
# extract the probabilites of each of the 40 terms
top_40_probs <- sort(blog_result$terms[topic_num,], decreasing=TRUE)[1:40]

# Visualizing, even though we don't have the stm plotting functions
library(wordcloud)
cloud_colors <- brewer.pal(8, "Dark2")
wordcloud(top_40_words, top_40_probs, random.order = FALSE, color=cloud_colors)

# Now let's try to get the expected topic proportions
# The data in blog_result$topics is what we need.
# Note that it is a 13246 x 20 matrix:
dim(blog_result$topics)
# So if we compute column means we will get the expected
# proportions for each of the 20 topics over all docs
# (Btw this is stolen from the stm code at
# https://github.com/bstewart/stm/blob/master/R/plot.STM.R )
exp_props <- colMeans(blog_result$topics)
print(exp_props)
invrank <- order(exp_props, decreasing=FALSE)
print(exp_props[invrank])

commas <- function(text){  
  paste(text[nchar(text)>0], collapse=", ")
}

topic.names <- sprintf("Topic %i:", 1:ncol(blog_result$topics))
# lol I can't believe just guessing a transpose actually worked
topic_labels <- apply(t(blog_terms), 1, commas)
lab <- sprintf("%s %s", topic.names, topic_labels)

# Except the STM code DIDN'T WORK as usual
# So instead I did this barplot thing stolen from
# https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781849513067/5/ch05lvl1sec11/placing-labels-inside-bars

y <- barplot(exp_props[invrank], main="Exp Props",
             horiz=TRUE, yaxt="n", space=0)
x<-0.5*exp_props[invrank]
text(x,y,lab[invrank])
```

# Bibliography